% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}

\begin{document}

\title{BMP-RAP: Branching Multithreaded Pipeline for Real-time Applications with Pooled Resources}
%\subtitle{}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{1} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Michael P. Scherer\\
       \affaddr{University of Central Florida}\\
       \affaddr{3100 Technology Parkway}\\
       \affaddr{Orlando, FL 32826}\\
       \email{michael.scherer@knights.ucf.edu}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
\date{12 March 2012}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
It is desirable to confirm the detection of objects or patterns in a computer vision problem using
multiple hand-tailored algorithms that work differently to get a result. This is as opposed to a system
such as boosting which uses a large number of weak classifiers that are
generated by the machine \cite{freund:boost}. The goal is to create a system which shall distribute work to m algorithms
running on n threads. Each algorithm may take a different amount of time to execute. Because this will
be for real-time applications, the system shall also guarantee a result within $\tau$ milliseconds,
regardless of the number of algorithms which have completed, and give a confidence value which reflects
this result. The end result will also be combined in meaningful way.
\end{abstract}

% A category with the (minimum) three required fields
\category{B.2.1}{Arithmetic and Logic Structures}{Design Styles}[parallel, pipeline]
\category{D.1}{Programming Techniques}{Concurrent Programming}

\terms{Algorithms, Design}

\keywords{concurrent tree, pipeline} % NOT required for Proceedings

%
%
%
\section{Introduction}
Concurrency can be very useful for real-time systems, however it can often be difficult to manage.
When dealing with robotic platforms, one may want to run data through multiple algorithms
in order to confirm a result. These algorithms may need to run simultaniously in order to take
advantage of the parallelism of multiple processes. In addition, there should be some mechanism
to get the best result available after some amount of time has passed.

BMP-RAP is based off of the pipe-and-filter paradigm, but also includes important features such
as thread and resource pooling, along with timeouts on finding the completed result. After time
$\tau$ has elapsed, the system gurantees to give the best result that it has at the time.

There are many problems, particularly in computer vision, which exibit such structure naturally.
For instance, most algorithms which employ a pyramid for analysis of an image in multiple scales
have a structure which can be modeled in this way. In such a program, an image is fed into the
system. It is then scaled successively by a factor, or alternatively some operation is performed
on it several times with an increasing factor, and then the image is further operated on at each
scale. At the end of the procedure, the results generated from processing each image is often
collapsed down to a single set of results \cite{lowe:sift,perona:scale}.

%
%
%
\subsection{Related Work}
The pipe-and-filter model was used as a general basis for the architecture of BMP-RAP. This is because
the model has very good properties when it comes to modularity, and because each filter can be
considered as a single automic operation \cite{shaw:pipes}. There have been many groups which have
worked on refining pipe-and-filter systems to make them optimal for various applications \cite{philipps:pipe_refine}.

There has also been a lot of work in pipelining a single algorithm used for data analysis in order
to take advantage of parallelism \cite{philipps:pipe_refine}. Unfortunately, these systems are limited
because they do not take into account pooled resources which can be shared, including threads, and
also are not designed for time-critical applications.

Fran\c{c}ois proposed a hybrid generic architecture for combining the benefits of a pipe and filter
model with shared resources \cite{francis:hybrid}. However, this system was designed such that
information would continuously flow into the pipe and be output at the end. This is in contrast
to the system being proposed where data flows in once per time interval, and must be processed by
a given deadline.


\section{Problem Statement}
Consider a program consisting of a tree of tasks \begin{math}t_0, t_1, ..., t_m\end{math} where each
task \begin{math}t_i\end{math} passes its result to all of its children. Each task can be considered
as a single automic operation on the data passed into it. All data passed into or out of a task is
immutable.

Every leaf node must either have the same type or be polymorphic to a single data type. At the end of an
execution cycle, the data returned from each leaf node \begin{math}\delta_0, \delta_1, ..., \delta_k\end{math}
can be combined using some algorithm into a final result $\delta$.

Each task can be executed on any thread \begin{math}T_i \in T_0, T_1, ..., T_n\end{math}. Regardless
of completion status, the algorithm must be guranteed to provide a result within $\tau$ milliseconds.
For non-time-critical applications, $\tau$ can be set to $\infty$

\subsection{Problem Analysis}
Clearly, this system can become bottlenecked at any place where threads need to wait for a single
node to complete execution before its children can be accessed. In the special case that the system
is used on an entirely serial set of branches, the system becomes useless.

%
%
%
\section{System Overview}
In essence, the system is performing a tree traversal, where at each visited node, some operation
is completed on it. For this system, there is a thread pool in order to mitigate the overhead
of creating and destroying threads during runtime. A queue of nodes is created to list which branches
are ready to be executed. The system is organized into three main classes: \begin{math}Tree\end{math},
\begin{math}Branch\end{math}, and \begin{math}Thread\end{math}. An instance of \begin{math}Tree\end{math}
serves as the root node, and also stores references to the thread pool. Branches are the nodes of the
tree, and are inherited from to create actual functionality.

For reasons of simplicity and rigor, all data passed from a node to its child
is considered to be immutable. Data is passed to a branch's children by returning it from said
branch's work function. The thread will then set each child's input to the output of the branch.
Finally, the thread adds each of the children to the node queue, and it starts operating on the
next branch in the queue.

\begin{figure}
\centering
\epsfig{file=flowdiagram.eps}
\caption{The process flow of a typical system}
\end{figure}

\subsection{Using the System}
To use the system, the user first creates an instance of Tree. When initialized, the user specifies the
number of threads to be available in the pool, along with a function pointer to a callback function, and
then begins adding child nodes to the tree. To add children, the user simply creates instances of branches
of different kinds to be pieced together for the tree. On each branch, the \begin{math}AddChild()\end{math}
function is called to add children to it. Children should only be added to Branches or the Tree before
execution time. To activate, the user calls \begin{math}SetJob\end{math} on the instance of the Tree.

\subsection{Implementing a Branch}
In order to implement a Branch, one would simply inherit from the Branch class. The user must then
overload a single function, and has the option to overload one additional method.
\begin{equation}virtual void* ExecuteJob()=0;\end{equation}
\begin{equation}virtual void Interrupt() {}\end{equation}
\begin{math}ExecuteJob()\end{math} gets called when a thread has taken control of the branch. This
function must be overloaded, and will do the actual work in the system. After the branch completes
its work, it should return the data it wishes to pass to its children. The user may also overload
the \begin{math}Interrupt\end{math} function if it wants to perform special clean-up behavior when
its work function gets interrupted because of a timeout.

\section{The Node Queue}
In order to increase parallelism of the system, it is desirable to use a queue for adding nodes which
is Wait-Free. For the purposes of this paper, a wait-free is defined algorithm as being able to finish
in a bound number of steps \cite{herlihy:art}. This is an ideal requirement, because it is desirable for
all threads to be working on each node as much as possible, rather than waiting to queue more nodes
onto the list.

\subsection{Wait-Free Node Queue Implementation}
The queue is implemented as a linked-list in order to make implementation simpler.
An array is defined which has one slot for each thread. When a thread wishes to enqueue an item, a
reference to said item is placed in its designated location. Before attempting to enqueue its own item,
it first attempts to help and enqueue its neighbor by executing a compare and swap on the tail of the
list with its neighbor. After its neighbor has been committed or if its neighbor has no information
to commit, then the add will attempt to commit its own item to the end of the queue.

When dequeing, a thread simply copies the head and then compare and swaps the head of the list with
its next element. If the CAS passes, then it has succesfully removed the head and can continue. Otherwise,
the dequeue operation will return NULL. The operation will also return NULL if the queue is empty. As
a desperate consumer, a thread would spin on the dequeue operation until there is work available, yielding
after a fixed number of attempts in order to allow other threads to do work and potentially enqueue
more nodes.

\section{Results}
The system was run on an Intel Quad-Core i7 with Hyperthreading, which had 8 logical cores. The machine
also had 8 GB of RAM. The tested algorithms were run using BMP-RAP and also run in serial, and their
run times were compared.

Actual data to be included...

%
%
%
\section{Conclusions}
To be included...

%
%
%
\section{Future Work}
To be included...

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
I would like to thank the hours and efforts that Dr. Damien Dechev
has put into making this class possible for all of us.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{scherer-midterm-paper}  % scherer-midterm-paper.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
% That's all folks!
\end{document}
